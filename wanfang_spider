host = "127.0.0.1"
port = 3306
user = "root"
passwd = "sdn"
db = 'repository'
key_words = ["河长制","智慧治水","水利厅","污水治理","一河一策","水政执法","水资源保护","河湖水域岸线保护","水污染防治","水生态修复","水利工程管理","环境改善","水质监测","涉河水利工程","岸线规划","污水治理","水环境治理"]

from pyspider.libs.base_handler import *
import time
import datetime
import pymysql
import re
import sys
reload(sys)
sys.setdefaultencoding('utf8')


class Handler(BaseHandler):
    
    crawl_config = {
    }

    @every(minutes=24 * 60 * 10)
    def on_start(self):
        for key in key_words:
            self.crawl('s.wanfangdata.com.cn/Paper.aspx?q='+key+'&f=top', callback=self.index_page)
            for i in range(2,50):
                self.crawl('s.wanfangdata.com.cn/Paper.aspx?q='+key+'&f=top&p='+str(i), callback=self.index_page)

    @config(age=24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc("a[href^='http://d.wanfangdata.com.cn/Periodical/']").items():
            self.crawl(each.attr.href , callback=self.detail_page)
                
    @config(priority=6)
    def detail_page(self, response):
        result = {}
        result["url"] = response.url
        result["title"] = response.doc('HTML>BODY>DIV.fixed-width.baseinfo.clear>DIV.section-baseinfo>h1').text()
        result["abstract"] = response.doc('HTML>BODY>DIV.fixed-width.baseinfo.clear>DIV.section-baseinfo>DIV.baseinfo-feild.abstract>DIV.row.clear.zh>DIV.text').text()
        result["author"] = response.doc('HTML>BODY>DIV.fixed-width-wrap.fixed-width-wrap-feild>DIV.fixed-width.baseinfo-feild>DIV.row.row-author>SPAN.text').text()
        result["magezine"] = response.doc('HTML>BODY>DIV.fixed-width-wrap.fixed-width-wrap-feild>DIV.fixed-width.baseinfo-feild>DIV.row.row-magazineName>SPAN.text').text()
        result["file_url"] = ''.join(x.attr.href for x in response.doc('HTML>BODY>DIV.fixed-width.baseinfo.clear>DIV.section-baseinfo>DIV.record-action-link.clear>A.download').items())
        result["keyword"] = response.doc('HTML>BODY>DIV.fixed-width-wrap.fixed-width-wrap-feild>DIV.fixed-width.baseinfo-feild>DIV.row.row-keyword>SPAN.text').text()
        result["spider_time"] = datetime.datetime.now()
        result["unit"] = ""
        result["time"] = ""
        for i in range(0,12):
            title = response.doc('HTML>BODY>DIV.fixed-width-wrap.fixed-width-wrap-feild>DIV.fixed-width.baseinfo-feild>DIV:nth-child('+str(i)+')>SPAN.pre').text()
            if title == "作者单位：":
                result["unit"] = response.doc('HTML>BODY>DIV.fixed-width-wrap.fixed-width-wrap-feild>DIV.fixed-width.baseinfo-feild>DIV:nth-child('+str(i)+')>SPAN.text').text()
            if title == "在线出版日期：":
                result["time"] = response.doc('HTML>BODY>DIV.fixed-width-wrap.fixed-width-wrap-feild>DIV.fixed-width.baseinfo-feild>DIV:nth-child('+str(i)+')>SPAN.text').text()
            
        return result
    
    def on_result(self,result):
        if not result or not result['title']:
            return
        conn= pymysql.connect(host=host,port=port,user=user,passwd=passwd,db=db,charset='utf8')
        cur = conn.cursor()
        
        #先查找是否存在
        cur.execute("select * from wanfang where url = %s" , result["url"])
        rows = cur.fetchall()
        if len(rows) == 0:
            cur.execute("insert into wanfang(title,abstract,author,unit,magezine,file_url,time,keyword,url,spider_time) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)" , (result["title"],result["abstract"],result["author"],result["unit"],result["magezine"],result["file_url"],result["time"],result["keyword"],result["url"],result["spider_time"]))
        conn.commit()
        cur.close()
        conn.close()
